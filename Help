Link : https://www.milanjovanovic.tech/blog/working-with-llms-in-dotnet-using-microsoft-extensions-ai?utm_source=YouTube&utm_medium=social&utm_campaign=03.02.2025#taking-it-further-smart-categorization


Getting Started
Before diving into the examples, here's what you need to run LLMs locally:

Docker running on your machine
Ollama container running with the llama3 model:

# Pull the Ollama container
docker run --gpus all -d -v ollama_data:/root/.ollama -p 11434:11434 --name ollama ollama/ollama ===> with Gpu
docker run  all -d -v ollama_data:/root/.ollama -p 11434:11434 --name ollama ollama/ollama ===> without Gpu


# Pull the llama3 model
docker exec -it ollama ollama pull llama3


Install-Package Microsoft.Extensions.AI # The base AI library
Install-Package Microsoft.Extensions.AI.Ollama # Ollama provider implementation
Install-Package Microsoft.Extensions.Hosting # For building the DI container
